{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSkEg7ddoFqu0rPIMryq6A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Servat0r/HLT-Project-2023/blob/master/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common utilities for the HLT project."
      ],
      "metadata": {
        "id": "oXoeziQs5tk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet \"transformers[sentencepiece]\" \"transformers[torch]\" datasets evaluate openai python-dotenv bert_score rouge_score bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "748tDOYqPOsP",
        "outputId": "bb476dd6-35f9-470d-9627-45733b435974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers[sentencepiece])\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[sentencepiece])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[sentencepiece])\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.66.1)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[sentencepiece]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[sentencepiece]) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]) (2023.7.22)\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.32.0\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.2/251.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.22.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.4 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.0 responses-0.18.0\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.9-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.9\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading and Preprocessing"
      ],
      "metadata": {
        "id": "ATo6RTQ7OL90"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa-r9zMvTjix"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def get_dataset(hf_url, local_train_path, local_eval_path, local_test_path):\n",
        "  if os.path.exists(local_train_path) and os.path.exists(local_eval_path) and os.path.exists(local_test_path):\n",
        "    train_dataset = load_from_disk(local_train_path)\n",
        "    eval_dataset = load_from_disk(local_eval_path)\n",
        "    test_dataset = load_from_disk(local_test_path)\n",
        "    return {'local': True, 'train': train_dataset, 'eval': eval_dataset, 'test': test_dataset}\n",
        "  else:\n",
        "    datasets = load_dataset(hf_url)\n",
        "    return {'local': False, 'all': datasets}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P848cy3e7yDc"
      },
      "outputs": [],
      "source": [
        "def get_maximum_labels_length(dataset, question_column='question'):\n",
        "  tokenized_dataset_lengths = [len(tokenizer.tokenize(sample)) for sample in dataset[question_column]]\n",
        "  return max(tokenized_dataset_lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tjPwntMvmFY",
        "outputId": "82851051-3d82-4b23-b280-3606f936480b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78\n"
          ]
        }
      ],
      "source": [
        "MAX_INPUTS_LENGTH = 512\n",
        "MAX_LABELS_LENGTH = 64\n",
        "def tokenizer_function(\n",
        "    samples, max_inputs_length=MAX_INPUTS_LENGTH, max_labels_length=MAX_LABELS_LENGTH,\n",
        "    input_ids_padding=True, train_dataset=None, ignore_index_id=-100,\n",
        "):\n",
        "  max_labels_length = max_labels_length if not train_dataset else get_maximum_labels_length(train_dataset)\n",
        "  input_tokenized = tokenizer(samples['answer_context'], padding=input_ids_padding, max_length=max_inputs_length, truncation=True, return_tensors='pt')\n",
        "  labels_tokenized = tokenizer(samples['question'], padding=\"max_length\", max_length=max_labels_length, truncation=True, return_tensors='pt')\n",
        "  labels, masks = labels_tokenized['input_ids'], labels_tokenized['attention_mask']\n",
        "  argmin_masks = torch.argmin(masks, dim=-1)\n",
        "  for index in range(len(argmin_masks)):\n",
        "    if masks[index][argmin_masks[index]] == 0 and ignore_index_id != 0:\n",
        "      labels[index][argmin_masks[index]:] = ignore_index_id\n",
        "  input_tokenized['labels'] = labels\n",
        "  return input_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HTTl7sUHe_n"
      },
      "outputs": [],
      "source": [
        "def build_answers_squad_it(sample, new_dataset):\n",
        "  answers_texts = list(set(sample['answers']['text']))\n",
        "  for answer_text in answers_texts:\n",
        "    new_dataset['answer'].append(answer_text)\n",
        "    new_dataset['question'].append(sample['question'])\n",
        "    new_dataset['context'].append(sample['context'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_train_feature(sample, use_extra_ids=False, context_label='paragraph_answer'):\n",
        "  return {'answer_context': f\"generate questions: <answer> {sample['answer']} <answer> <context> {sample[context_label]} <context>\"}"
      ],
      "metadata": {
        "id": "DDuEkYvTUvQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP6EfSoXuXuL"
      },
      "outputs": [],
      "source": [
        "def build_train_feature_squad_it(sample, use_extra_ids=False):\n",
        "  return build_train_feature(sample, use_extra_ids=use_extra_ids, context_label='context')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_squad_it_dataset(\n",
        "    dataset_name='squad_it', train_dataset_name='squad_it_qg_train',\n",
        "    eval_dataset_name='squad_it_qg_eval', test_dataset_name='squad_it_qg_test',\n",
        "    shuffle_seed=None, train_select=None, eval_select=None, train_dataset_split=0.8,\n",
        "    use_extra_ids=False,\n",
        "):\n",
        "  dataset_loading_result = get_dataset(dataset_name, train_dataset_name, eval_dataset_name, test_dataset_name)\n",
        "  local = dataset_loading_result['local']\n",
        "  if local:\n",
        "    train_dataset = dataset_loading_result['train']\n",
        "    validation_dataset = dataset_loading_result['eval']\n",
        "    test_dataset = dataset_loading_result['test']\n",
        "  if not local:\n",
        "    datasets = load_dataset(dataset_name)\n",
        "    dev_dataset = datasets['train'].remove_columns(['id'])\n",
        "    test_dataset = datasets['test'].remove_columns(['id'])\n",
        "\n",
        "    new_test_dataset = {'answer': [], 'question': [], 'context': []}\n",
        "    test_dataset.map(lambda sample: build_answers_squad_it(sample, new_test_dataset))\n",
        "    test_dataset = Dataset.from_dict(new_test_dataset)\n",
        "\n",
        "    new_dev_dataset = {'answer': [], 'question': [], 'context': []}\n",
        "    dev_dataset.map(lambda sample: build_answers_squad_it(sample, new_dev_dataset))\n",
        "    del dev_dataset\n",
        "    dev_dataset = Dataset.from_dict(new_dev_dataset)\n",
        "\n",
        "    train_dataset_length = int(train_dataset_split * dev_dataset_length) + 1 if 0 <= train_dataset_split <= 1 else int(train_dataset_split)\n",
        "\n",
        "    train_dataset = dev_dataset.shuffle(seed=shuffle_seed).select(range(train_dataset_length))\n",
        "    validation_dataset = dev_dataset.shuffle(seed=shuffle_seed).select(range(train_dataset_length, dev_dataset_length))\n",
        "    print(f\"Train dataset has {len(train_dataset)} items. Validation dataset has {len(validation_dataset)} items.\")\n",
        "\n",
        "    train_dataset.save_to_disk(train_dataset_name)\n",
        "    validation_dataset.save_to_disk(eval_dataset_name)\n",
        "    test_dataset.save_to_disk(test_dataset_name)\n",
        "\n",
        "  if train_select:\n",
        "    train_dataset = train_dataset.shuffle(seed=shuffle_seed).select(range(train_select))\n",
        "  if eval_select:\n",
        "    validation_dataset = validation_dataset.shuffle(seed=shuffle_seed).select(range(eval_select))\n",
        "\n",
        "  build_train_feature = lambda sample: build_train_feature_squad_it(sample, use_extra_ids=use_extra_ids)\n",
        "  train_dataset = train_dataset.map(build_train_feature).remove_columns(['answer', 'context'])\n",
        "  validation_dataset = validation_dataset.map(build_train_feature).remove_columns(['answer', 'context'])\n",
        "  test_dataset = test_dataset.map(build_train_feature).remove_columns(['answer', 'context'])\n",
        "\n",
        "  tokenized_train_dataset = train_dataset.map(tokenizer_function, batched=True).remove_columns(['answer_context', 'question'])\n",
        "  tokenized_validation_dataset = validation_dataset.map(tokenizer_function, batched=True).remove_columns(['answer_context', 'question'])\n",
        "  tokenized_test_dataset = test_dataset.map(lambda samples: tokenizer_function(samples, input_ids_padding=\"max_length\", train_dataset=train_dataset), batched=True).remove_columns(['answer_context'])\n",
        "\n",
        "  tokenized_train_dataset.set_format(\"torch\")\n",
        "  tokenized_validation_dataset.set_format(\"torch\")\n",
        "  tokenized_test_dataset.set_format(\"torch\")\n",
        "\n",
        "  return (train_dataset, validation_dataset, test_dataset), (tokenized_train_dataset, tokenized_validation_dataset, tokenized_test_dataset)"
      ],
      "metadata": {
        "id": "8Ot4o0nD6vz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_train_feature_lmqg_squad(sample, use_extra_ids=False):\n",
        "  return build_train_feature(sample, use_extra_ids=use_extra_ids, context_label='paragraph')"
      ],
      "metadata": {
        "id": "3Acsa-2HLijU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_lmqg_squad_dataset(\n",
        "    dataset_name='lmqg/qg_squad', train_dataset_name='lmqg_qg_squad_train',\n",
        "    eval_dataset_name='lmqg_qg_squad_eval', test_dataset_name='lmqg_qg_squad_test',\n",
        "    shuffle_seed=None, train_select=None, eval_select=None, use_extra_ids=False,\n",
        "):\n",
        "  dataset_loading_result = get_dataset(dataset_name, train_dataset_name, eval_dataset_name, test_dataset_name)\n",
        "  local = dataset_loading_result['local']\n",
        "  if local:\n",
        "    train_dataset = dataset_loading_result['train']\n",
        "    validation_dataset = dataset_loading_result['eval']\n",
        "    test_dataset = dataset_loading_result['test']\n",
        "  if not local:\n",
        "    datasets = load_dataset(dataset_name)\n",
        "    train_dataset = datasets['train'].remove_columns(['paragraph_question', 'sentence', 'sentence_answer', 'paragraph_answer', 'paragraph_sentence'])\n",
        "    validation_dataset = datasets['validation'].remove_columns(['paragraph_question', 'sentence', 'sentence_answer', 'paragraph_answer', 'paragraph_sentence'])\n",
        "    test_dataset = datasets['test'].remove_columns(['paragraph_question', 'sentence', 'sentence_answer', 'paragraph_answer', 'paragraph_sentence'])\n",
        "    print(f\"Train dataset has {len(train_dataset)} items. Validation dataset has {len(validation_dataset)} items.\")\n",
        "\n",
        "    train_dataset.save_to_disk(train_dataset_name)\n",
        "    validation_dataset.save_to_disk(eval_dataset_name)\n",
        "    test_dataset.save_to_disk(test_dataset_name)\n",
        "\n",
        "  if train_select:\n",
        "    train_dataset = train_dataset.shuffle(seed=0).select(range(train_select))\n",
        "  if eval_select:\n",
        "    validation_dataset = validation_dataset.shuffle(seed=0).select(range(eval_select))\n",
        "\n",
        "  build_train_feature = lambda sample: build_train_feature_lmqg_squad(sample, use_extra_ids=use_extra_ids)\n",
        "  train_dataset = train_dataset.map(build_train_feature).remove_columns(['answer', 'paragraph'])\n",
        "  validation_dataset = validation_dataset.map(build_train_feature).remove_columns(['answer', 'paragraph'])\n",
        "  test_dataset = test_dataset.map(build_train_feature).remove_columns(['answer', 'paragraph'])\n",
        "\n",
        "  tokenized_train_dataset = train_dataset.map(tokenizer_function, batched=True).remove_columns(['answer_context', 'question'])\n",
        "  tokenized_validation_dataset = validation_dataset.map(tokenizer_function, batched=True).remove_columns(['answer_context', 'question'])\n",
        "  tokenized_test_dataset = test_dataset.map(lambda samples: tokenizer_function(samples, input_ids_padding=\"max_length\"), batched=True).remove_columns(['answer_context'])\n",
        "\n",
        "  tokenized_train_dataset.set_format(\"torch\")\n",
        "  tokenized_validation_dataset.set_format(\"torch\")\n",
        "  tokenized_test_dataset.set_format(\"torch\")\n",
        "\n",
        "  return (train_dataset, validation_dataset, test_dataset), (tokenized_train_dataset, tokenized_validation_dataset, tokenized_test_dataset)"
      ],
      "metadata": {
        "id": "mHmunHnI6zmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_train_feature_lmqg_squad_highlighting(sample, use_extra_ids=False):\n",
        "  return build_train_feature(sample, use_extra_ids=use_extra_ids, context_label='paragraph_answer')"
      ],
      "metadata": {
        "id": "6JIgqlatGFnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_lmqg_squad_dataset_highlighting(\n",
        "    dataset_name='lmqg/qg_squad', train_dataset_name='lmqg_qg_squad_highlighting_train',\n",
        "    eval_dataset_name='lmqg_qg_squad_highlighting_eval', test_dataset_name='lmqg_qg_squad_highlighting_test',\n",
        "    shuffle_seed=None, train_select=None, eval_select=None, use_extra_ids=False,\n",
        "):\n",
        "  dataset_loading_result = get_dataset(dataset_name, train_dataset_name, eval_dataset_name, test_dataset_name)\n",
        "  local = dataset_loading_result['local']\n",
        "  if local:\n",
        "    train_dataset = dataset_loading_result['train']\n",
        "    validation_dataset = dataset_loading_result['eval']\n",
        "    test_dataset = dataset_loading_result['test']\n",
        "  if not local:\n",
        "    datasets = load_dataset(dataset_name)\n",
        "    print(datasets['test'])\n",
        "    train_dataset = datasets['train'].remove_columns(['paragraph_question', 'sentence', 'sentence_answer', 'paragraph', 'paragraph_sentence'])\n",
        "    validation_dataset = datasets['validation'].remove_columns(['paragraph_question', 'sentence', 'sentence_answer', 'paragraph', 'paragraph_sentence'])\n",
        "    test_dataset = datasets['test'].remove_columns(['paragraph_question', 'sentence', 'sentence_answer', 'paragraph', 'paragraph_sentence'])\n",
        "    print(f\"Train dataset has {len(train_dataset)} items. Validation dataset has {len(validation_dataset)} items.\")\n",
        "\n",
        "    train_dataset.save_to_disk(train_dataset_name)\n",
        "    validation_dataset.save_to_disk(eval_dataset_name)\n",
        "    test_dataset.save_to_disk(test_dataset_name)\n",
        "\n",
        "  if train_select:\n",
        "    train_dataset = train_dataset.shuffle(seed=0).select(range(train_select))\n",
        "  if eval_select:\n",
        "    validation_dataset = validation_dataset.shuffle(seed=0).select(range(eval_select))\n",
        "\n",
        "  build_train_feature = lambda sample: build_train_feature_lmqg_squad_highlighting(sample, use_extra_ids=use_extra_ids)\n",
        "  train_dataset = train_dataset.map(build_train_feature).remove_columns(['answer', 'paragraph_answer'])\n",
        "  validation_dataset = validation_dataset.map(build_train_feature).remove_columns(['answer', 'paragraph_answer'])\n",
        "  test_dataset = test_dataset.map(build_train_feature).remove_columns(['answer', 'paragraph_answer'])\n",
        "\n",
        "  tokenizer_function_lambda = lambda sample: tokenizer_function(sample, train_dataset=train_dataset)\n",
        "  tokenized_train_dataset = train_dataset.map(tokenizer_function_lambda, batched=True).remove_columns(['answer_context', 'question'])\n",
        "  tokenized_validation_dataset = validation_dataset.map(tokenizer_function_lambda, batched=True).remove_columns(['answer_context', 'question'])\n",
        "  tokenized_test_dataset = test_dataset.map(lambda samples: tokenizer_function(samples, input_ids_padding=\"max_length\"), batched=True).remove_columns(['answer_context', 'question'])\n",
        "\n",
        "  tokenized_train_dataset.set_format(\"torch\")\n",
        "  tokenized_validation_dataset.set_format(\"torch\")\n",
        "  tokenized_test_dataset.set_format(\"torch\")\n",
        "\n",
        "  return (train_dataset, validation_dataset, test_dataset), (tokenized_train_dataset, tokenized_validation_dataset, tokenized_test_dataset)"
      ],
      "metadata": {
        "id": "0-OwFw_NRNYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_squad_qg_dataset(\n",
        "    dataset_name='derek-thomas/squad-v1.1-t5-question-generation', train_dataset_name='squad_qg_train',\n",
        "    eval_dataset_name='squad_qg_eval', test_dataset_name='squad_qg_test', shuffle_seed=None,\n",
        "    train_select=None, eval_select=None, use_extra_ids=False, eval_split=0.3,\n",
        "):\n",
        "  dataset_loading_result = get_dataset(dataset_name, train_dataset_name, eval_dataset_name, test_dataset_name)\n",
        "  local = dataset_loading_result['local']\n",
        "  if local:\n",
        "    train_dataset = dataset_loading_result['train']\n",
        "    test_dataset = dataset_loading_result['test']\n",
        "  if not local:\n",
        "    datasets = load_dataset(dataset_name)\n",
        "    dev_dataset, test_dataset = datasets['train'], datasets['validation']\n",
        "    print(f\"Dev dataset has {len(dev_dataset)} items. Test dataset has {len(test_dataset)} items.\")\n",
        "\n",
        "    eval_length = int(0.2 * len(dev_dataset))\n",
        "    train_length = len(dev_dataset) - eval_length\n",
        "\n",
        "    dev_dataset = dev_dataset.shuffle(seed=shuffle_seed)\n",
        "    train_dataset = dev_dataset.select(range(train_length))\n",
        "    validation_dataset = dev_dataset.select(range(train_length, train_length + eval_length))\n",
        "\n",
        "    train_dataset.save_to_disk(train_dataset_name)\n",
        "    validation_dataset.save_to_disk(eval_dataset_name)\n",
        "    test_dataset.save_to_disk(test_dataset_name)\n",
        "\n",
        "  if train_select:\n",
        "    train_dataset = train_dataset.shuffle(seed=0).select(range(train_select))\n",
        "  if eval_select:\n",
        "    validation_dataset = validation_dataset.shuffle(seed=0).select(range(eval_select))\n",
        "\n",
        "  tokenized_train_dataset = train_dataset.map(tokenizer_function, batched=True).remove_columns(['context', 'question'])\n",
        "  tokenized_validation_dataset = validation_dataset.map(tokenizer_function, batched=True).remove_columns(['context', 'question'])\n",
        "  tokenized_test_dataset = test_dataset.map(lambda samples: tokenizer_function(samples, input_ids_padding=\"max_length\"), batched=True).remove_columns(['context', 'question'])\n",
        "\n",
        "  tokenized_train_dataset.set_format(\"torch\")\n",
        "  tokenized_validation_dataset.set_format(\"torch\")\n",
        "  tokenized_test_dataset.set_format(\"torch\")\n",
        "\n",
        "  return (train_dataset, validation_dataset, test_dataset), (tokenized_train_dataset, tokenized_validation_dataset, tokenized_test_dataset)"
      ],
      "metadata": {
        "id": "ijQcr2_acKh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metric loading, configuration and computation"
      ],
      "metadata": {
        "id": "iHg7Mw6iPK-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def std_conversion_predictions(data, tokenizer):\n",
        "  return tokenizer.batch_decode(data, skip_special_tokens=True)\n",
        "\n",
        "def std_conversion_references(data, tokenizer):\n",
        "  data[data == -100] = tokenizer.pad_token_id\n",
        "  return [[reference] for reference in tokenizer.batch_decode(data, skip_special_tokens=True)]\n",
        "\n",
        "def get_bleu_config(tokenizer):\n",
        "  return evaluate.load('bleu'), 'text', 'text'\n",
        "\n",
        "def get_nist_config(tokenizer):\n",
        "  return evaluate.load('nist_mt'), 'text', 'text'\n",
        "\n",
        "def get_rouge_config(tokenizer):\n",
        "  return evaluate.load('rouge'), 'text', 'text'"
      ],
      "metadata": {
        "id": "bY-XSAwDPNKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Configuration"
      ],
      "metadata": {
        "id": "sU0KDgA2O0Ef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8qOzXBLBUxf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import get_scheduler\n",
        "\n",
        "def get_training_configuration(\n",
        "    *, optimizer='adam', learning_rate=1e-3, train_collate_fn=None,\n",
        "    eval_collate_fn=None, tokenizer=None, train_batch_size=8, eval_batch_size=8,\n",
        "    num_epochs=3, lr_scheduler='linear', num_warmup_steps=0,\n",
        "):\n",
        "  # Handle optimizer\n",
        "  if optimizer == 'adam':\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "  elif isinstance(optimizer, str):  # Allow for other optimizer objects defined outside to be used\n",
        "    raise ValueError(f\"Unknown optimizer: '{optimizer}'\")\n",
        "  # Handle collators\n",
        "  data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "  train_collate_fn = train_collate_fn if train_collate_fn else data_collator\n",
        "  eval_collate_fn = train_collate_fn if train_collate_fn else data_collator\n",
        "  # Dataloaders\n",
        "  train_dataloader = DataLoader(\n",
        "      tokenized_train_dataset, shuffle=True, batch_size=train_batch_size, collate_fn=train_collate_fn)\n",
        "  eval_dataloader = DataLoader(\n",
        "      tokenized_validation_dataset, batch_size=eval_batch_size, collate_fn=eval_collate_fn)\n",
        "  # Handle scheduling\n",
        "  num_training_steps = num_epochs * len(train_dataloader)\n",
        "  print(num_training_steps)\n",
        "  if lr_scheduler == 'linear':\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps,\n",
        "    )\n",
        "  elif isinstance(lr_scheduler, str):  # As before for optimizers\n",
        "    raise ValueError(f\"Unknown scheduler: '{lr_scheduler}'\")\n",
        "  return optimizer, train_dataloader, eval_dataloader, lr_scheduler, num_training_steps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning"
      ],
      "metadata": {
        "id": "4cs6E4GsO_Za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from evaluate import load\n",
        "bertscore = load('bertscore')\n",
        "def bertscore_f1based_score(grouped_predictions, references, verbose=False):\n",
        "  m, n = len(grouped_predictions), len(grouped_predictions[0])\n",
        "  scores = np.zeros((m, n), dtype=np.float64)\n",
        "  grouped_predictions = np.array(grouped_predictions)\n",
        "  for i in range(n):\n",
        "    current_predictions = grouped_predictions[:, i]\n",
        "    if verbose:\n",
        "      print(\"Current Predictions:\", current_predictions, \"Current References:\", references, '\\n', sep='\\n')\n",
        "    current_scores = bertscore.compute(predictions=current_predictions, references=references, lang='en')\n",
        "    scores[:, i] = current_scores['f1']\n",
        "  return scores"
      ],
      "metadata": {
        "id": "qMTa-7rRpw_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def select_best_output(\n",
        "    model, tokenizer, input_ids, references, score_function, max_length, num_beams, top_k, top_p, num_candidates,\n",
        "    verbose=False, tokenize_output=False,\n",
        "):\n",
        "  num_sequences = len(input_ids)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    predictions = model.generate(input_ids, max_length=max_length, num_beams=num_beams, top_k=top_k, top_p=top_p, num_return_sequences=num_candidates)\n",
        "  decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "  grouped_predictions = []\n",
        "  for i in range(0, len(decoded_predictions), num_beams):\n",
        "    grouped_predictions.append(decoded_predictions[i:i+num_beams])\n",
        "  scores = score_function(grouped_predictions, references, verbose=verbose)\n",
        "  if verbose:\n",
        "    print(scores)\n",
        "  scores = np.argmax(scores, axis=-1)\n",
        "  final_predictions = [item_predictions[score_index] for item_predictions, score_index in zip(grouped_predictions, scores)]\n",
        "  if tokenize_output:\n",
        "    final_predictions = tokenizer(final_predictions, padding=True, max_length=512, truncation=True, return_tensors='pt')\n",
        "  return final_predictions"
      ],
      "metadata": {
        "id": "oFdlsPwAkxqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nN7V1bzd2xhx"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "def evaluation_loop(\n",
        "    model, device, optimizer, eval_dataloader, lr_scheduler,\n",
        "    loss_tracker, metrics_tracker=None, metrics=None, progress_bar=None,\n",
        "    tokenizer=None, num_beams=1, top_k=None, top_p=None, num_candidates=4,\n",
        "    score_function=bertscore_f1based_score, tokenize_predictions_output=True,\n",
        "):\n",
        "    model.eval()\n",
        "    current_loss = 0\n",
        "    for batch in eval_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        if 'question' in batch:\n",
        "          text_references = batch['question']\n",
        "        else:\n",
        "          labels_batch = torch.tensor(batch['labels'])\n",
        "          labels_batch[labels_batch == -100] = tokenizer.pad_token_id\n",
        "          text_references = tokenizer.batch_decode(labels_batch, skip_special_tokens=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        current_loss = outputs.loss.item()\n",
        "        if metrics:\n",
        "          predictions = select_best_output(\n",
        "              model, tokenizer, batch['input_ids'], text_references, score_function, max_length=100, num_beams=num_beams,\n",
        "              top_k=top_k, top_p=top_p, num_candidates=num_candidates, verbose=False, tokenize_output=tokenize_predictions_output,\n",
        "          )\n",
        "          for metric_name, (metric, conversion_function_predictions, conversion_function_references) in metrics.items():\n",
        "            conversion_function_predictions = std_conversion_predictions if conversion_function_predictions == 'text' else conversion_function_predictions\n",
        "            converted_predictions = conversion_function_predictions(predictions) if tokenize_predictions_output else predictions\n",
        "            references = text_references if conversion_function_references == 'text' else conversion_function_references(batch[\"labels\"])\n",
        "            metric.add_batch(predictions=converted_predictions, references=references)\n",
        "        if progress_bar:\n",
        "          progress_bar.update(1)\n",
        "    loss_tracker.append(current_loss)\n",
        "    if metrics:\n",
        "      metrics_tracker.append({\n",
        "        metric_name: metric.compute() for metric_name, (metric, _, _) in metrics.items()\n",
        "      })\n",
        "      print(f\"Metrics = {metrics_tracker[-1]}\")\n",
        "    return current_loss\n",
        "\n",
        "\n",
        "def main_training_loop(\n",
        "    model, device, optimizer, train_dataloader, eval_dataloader,\n",
        "    lr_scheduler, num_training_steps, num_epochs, metrics=None,\n",
        "    eval_strategy='epoch', eval_every=1000, model_save_path='model',\n",
        "    early_stopping=False, early_stopping_min_delta=1e-3, early_stopping_patience=5,\n",
        "    tokenizer=None, num_beams=1, top_k=None, top_p=None, num_candidates=4,\n",
        "    score_function=bertscore_f1based_score, tokenize_predictions_output=True,\n",
        "    start_epoch=0,\n",
        "):\n",
        "  if eval_strategy == 'epoch':\n",
        "    num_evaluation_steps = num_epochs * len(eval_dataloader)\n",
        "  elif eval_strategy == 'steps':\n",
        "    num_evaluation_steps = (num_training_steps // eval_every + num_epochs * len(eval_dataloader)) * len(eval_dataloader)\n",
        "  elif (not eval_strategy) or (eval_strategy == 'no'):\n",
        "    num_evaluation_steps = 0\n",
        "  else:\n",
        "    raise ValueError(f\"Unknown evaluation strategy: '{eval_strategy}'\")\n",
        "  _flag_evaluate_steps = eval_strategy == 'steps'\n",
        "  _flag_evaluate_epochs = eval_strategy == 'epoch'\n",
        "\n",
        "  training_progress_bar = tqdm(range(num_training_steps))\n",
        "  evaluation_progress_bar = tqdm(range(num_evaluation_steps)) if num_evaluation_steps > 0 else None\n",
        "\n",
        "  batch_train_losses = []\n",
        "  epoch_train_losses = []\n",
        "  epoch_eval_losses = []\n",
        "  steps_eval_losses = []\n",
        "  epoch_eval_metrics = []\n",
        "  steps_eval_metrics = []\n",
        "  step = 1\n",
        "  # Early Stopping parameters\n",
        "  best_early_stopping_value = float('inf')\n",
        "  best_early_stopping_checkpoint = 0\n",
        "  best_early_stopping_type = None\n",
        "  current_early_stopping_patience = 0\n",
        "\n",
        "  stop_training = False\n",
        "  for epoch in range(start_epoch, num_epochs + start_epoch):\n",
        "      if stop_training:\n",
        "        break\n",
        "      model.train()\n",
        "      current_loss = 0\n",
        "      batch_train_losses.append([])\n",
        "      for batch in train_dataloader:\n",
        "          if stop_training:\n",
        "            break\n",
        "          batch = {k: v.to(device) for k, v in batch.items()}\n",
        "          outputs = model(**batch)\n",
        "          loss = outputs.loss\n",
        "          current_loss = loss.item()\n",
        "          batch_train_losses[-1].append(current_loss)\n",
        "          loss.backward()\n",
        "\n",
        "          optimizer.step()\n",
        "          lr_scheduler.step()\n",
        "          optimizer.zero_grad()\n",
        "          training_progress_bar.update(1)\n",
        "          if step % eval_every == 0 and _flag_evaluate_steps:\n",
        "            eval_loss = evaluation_loop(\n",
        "                model, device, optimizer, eval_dataloader, lr_scheduler,\n",
        "                steps_eval_losses, steps_eval_metrics, metrics, evaluation_progress_bar,\n",
        "                tokenizer=tokenizer, num_beams=num_beams, top_k=top_k, top_p=top_k, num_candidates=num_candidates,\n",
        "                score_function=score_function, tokenize_predictions_output=tokenize_predictions_output,\n",
        "            )\n",
        "            if metrics:\n",
        "              print(f\"Step {step}: Train Loss = {current_loss}, Eval Loss = {eval_loss}, Metrics = {steps_eval_metrics[-1]}\")\n",
        "            else:\n",
        "              print(f\"Step {step}: Train Loss = {current_loss}, Eval Loss = {eval_loss}\")\n",
        "            try:\n",
        "              save_model = input('Save this model (y/n)?> ')\n",
        "              save_model = save_model == 'y'\n",
        "            except:\n",
        "              save_model = False\n",
        "            if save_model:\n",
        "              model.save_pretrained(f'{model_save_path}_step{step}')\n",
        "            try:\n",
        "              continue_training = input('Continue training (y/n)?> ')\n",
        "              continue_training = continue_training == 'y'\n",
        "            except:\n",
        "              continue_training = True\n",
        "            stop_training = not continue_training\n",
        "            if eval_loss + early_stopping_min_delta <= best_early_stopping_value:\n",
        "              best_early_stopping_value = eval_loss\n",
        "              best_early_stopping_checkpoint = step\n",
        "              best_early_stopping_type = 'step'\n",
        "              current_early_stopping_patience = 0\n",
        "            else:\n",
        "              current_early_stopping_patience += 1\n",
        "              if current_early_stopping_patience >= early_stopping_patience:\n",
        "                stop_training = True\n",
        "            model.train()\n",
        "          step += 1\n",
        "      if stop_training:\n",
        "        break\n",
        "      torch.cuda.synchronize()  # Sure?\n",
        "      epoch_train_losses.append(current_loss)\n",
        "      # If we don't set to evaluate to epoch, we however do an evaluation steps to register epoch loss\n",
        "      epoch_eval_metrics_now = epoch_eval_metrics if _flag_evaluate_epochs else None\n",
        "      metrics_now = metrics if _flag_evaluate_epochs else None\n",
        "      eval_loss = evaluation_loop(\n",
        "          model, device, optimizer, eval_dataloader, lr_scheduler,\n",
        "          epoch_eval_losses, epoch_eval_metrics_now, metrics_now, evaluation_progress_bar,\n",
        "          tokenizer=tokenizer, num_beams=num_beams, top_k=top_k, top_p=top_k, num_candidates=num_candidates,\n",
        "          score_function=score_function, tokenize_predictions_output=tokenize_predictions_output,\n",
        "      )\n",
        "      if metrics_now:\n",
        "        print(f\"Epoch {epoch}: Train Loss = {current_loss}, Eval Loss = {epoch_eval_losses[-1]}, Metrics = {epoch_eval_metrics[-1]}\")\n",
        "      else:\n",
        "        print(f\"Epoch {epoch}: Train Loss = {current_loss}, Eval Loss = {epoch_eval_losses[-1]}\")\n",
        "      if eval_loss + early_stopping_min_delta <= best_early_stopping_value:\n",
        "        best_early_stopping_value = eval_loss\n",
        "        best_early_stopping_checkpoint = epoch\n",
        "        best_early_stopping_type = 'epoch'\n",
        "        current_early_stopping_patience = 0\n",
        "      else:\n",
        "        current_early_stopping_patience += 1\n",
        "        if current_early_stopping_patience >= early_stopping_patience:\n",
        "          stop_training = True\n",
        "      try:\n",
        "        save_model = input('Save this model (y/n)?> ')\n",
        "        save_model = save_model == 'y'\n",
        "      except:\n",
        "        save_model = False\n",
        "      if save_model:\n",
        "        model.save_pretrained(f'{model_save_path}_epoch{epoch}')\n",
        "      try:\n",
        "        continue_training = input('Continue training (y/n)?> ')\n",
        "        continue_training = continue_training == 'y'\n",
        "      except:\n",
        "        continue_training = True\n",
        "      stop_training = not continue_training\n",
        "  model.eval()\n",
        "  try:\n",
        "    return {\n",
        "        'batch_train_losses': batch_train_losses,\n",
        "        'epoch_train_losses': epoch_train_losses,\n",
        "        'epoch_eval_losses': epoch_eval_losses,\n",
        "        'steps_eval_losses': steps_eval_losses,\n",
        "        'epoch_eval_metrics': epoch_eval_metrics,\n",
        "        'steps_eval_metrics': steps_eval_metrics,\n",
        "        'best_early_stopping_checkpoint': best_early_stopping_checkpoint,\n",
        "        'best_early_stopping_type': best_early_stopping_type,\n",
        "        'epoch': epoch,\n",
        "    }\n",
        "  except:\n",
        "    return {\n",
        "        'batch_train_losses': batch_train_losses,\n",
        "        'epoch_train_losses': epoch_train_losses,\n",
        "        'epoch_eval_losses': epoch_eval_losses,\n",
        "        'steps_eval_losses': steps_eval_losses,\n",
        "        'epoch_eval_metrics': epoch_eval_metrics,\n",
        "        'steps_eval_metrics': steps_eval_metrics,\n",
        "        'best_early_stopping_checkpoint': best_early_stopping_checkpoint,\n",
        "        'best_early_stopping_type': best_early_stopping_type,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checkpointing and resuming"
      ],
      "metadata": {
        "id": "BtbUIbGC_KoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taken from: https://discuss.pytorch.org/t/moving-optimizer-from-cpu-to-gpu/96068/2\n",
        "def optimizer_to(optim, device):\n",
        "    for param in optim.state.values():\n",
        "        # Not sure there are any global tensors in the state dict\n",
        "        if isinstance(param, torch.Tensor):\n",
        "            param.data = param.data.to(device)\n",
        "            if param._grad is not None:\n",
        "                param._grad.data = param._grad.data.to(device)\n",
        "        elif isinstance(param, dict):\n",
        "            for subparam in param.values():\n",
        "                if isinstance(subparam, torch.Tensor):\n",
        "                    subparam.data = subparam.data.to(device)\n",
        "                    if subparam._grad is not None:\n",
        "                        subparam._grad.data = subparam._grad.data.to(device)"
      ],
      "metadata": {
        "id": "wr9q0ncrGbAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(checkpoint_path, model_path, optimizer, lr_scheduler, num_training_steps, model=None, save_model=False):\n",
        "  checkpoint = {\n",
        "      'model_path': model_path,\n",
        "      'optimizer': optimizer.state_dict(),\n",
        "      'lr_scheduler': lr_scheduler.state_dict(),\n",
        "  }\n",
        "  checkpoint['num_training_steps'] = num_training_steps - checkpoint['lr_scheduler']['_step_count'] + 1\n",
        "  if save_model:\n",
        "    checkpoint['model'] = model\n",
        "  torch.save(checkpoint, checkpoint_path)"
      ],
      "metadata": {
        "id": "cX3I8E8a_KBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(checkpoint_path, device, optimizer, lr_scheduler, model_class, new_model_path=None):\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "  if new_model_path:\n",
        "    checkpoint['model_path'] = new_model_path\n",
        "  model = model_class.from_pretrained(checkpoint['model_path'], local_files_only=True)\n",
        "  model.to(device)\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  num_training_steps = checkpoint['num_training_steps']\n",
        "  lr_scheduler_dict = checkpoint['lr_scheduler']\n",
        "  lr_scheduler.base_lrs = lr_scheduler_dict['base_lrs']\n",
        "  lr_scheduler.last_epoch = lr_scheduler_dict['last_epoch']\n",
        "  lr_scheduler._step_count = lr_scheduler_dict['_step_count']\n",
        "  lr_scheduler._get_lr_called_within_step = lr_scheduler_dict['_get_lr_called_within_step']\n",
        "  lr_scheduler._last_lr = lr_scheduler_dict['_last_lr']\n",
        "  if device != torch.device('cpu'):\n",
        "    optimizer_to(optimizer, device)\n",
        "  return model, optimizer, lr_scheduler, num_training_steps"
      ],
      "metadata": {
        "id": "q4ZWEwepAyyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics"
      ],
      "metadata": {
        "id": "oOwF4ZlEbLTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "accuracy = load('accuracy')\n",
        "test_loss_tracker=[]\n",
        "test_metrics_tracker=[]\n",
        "def accuracy_conversion_function(predictions):\n",
        "  predicted_labels = torch.argmax(predictions, dim=-1)\n",
        "  return predicted_labels"
      ],
      "metadata": {
        "id": "xD_IscsWECll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import top_k_accuracy_score\n",
        "top_2_accuracy = lambda y_score, y_true: top_k_accuracy_score(y_true, y_score, k=2, labels=np.arange(13))\n",
        "top_3_accuracy = lambda y_score, y_true: top_k_accuracy_score(y_true, y_score, k=3, labels=np.arange(13))"
      ],
      "metadata": {
        "id": "ewjpaFkJTEsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_accuracy_conversion_function_predictions(logits):\n",
        "  probabilities = torch.softmax(logits, dim=-1)\n",
        "  return probabilities.clone().detach().cpu()\n",
        "\n",
        "def top_k_accuracy_conversion_function_references(references):\n",
        "  return references.cpu() # ????"
      ],
      "metadata": {
        "id": "-EauuT2KV4x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TopKAccuracyMetric:\n",
        "\n",
        "  def __init__(self, k, num_classes):\n",
        "    self.k = k\n",
        "    self.num_classes = num_classes\n",
        "    self.score_function = lambda y_score, y_true: top_k_accuracy_score(y_true, y_score, k=k, labels=np.arange(num_classes))\n",
        "    self.batches = []\n",
        "\n",
        "  def add_batch(self, predictions, references):\n",
        "    score = self.score_function(predictions, references)\n",
        "    self.batches.append(score)\n",
        "\n",
        "  def compute(self, *args, **kwargs):\n",
        "    result = np.mean(self.batches)\n",
        "    self.clear()\n",
        "    return result\n",
        "\n",
        "  def clear(self):\n",
        "    self.batches.clear()"
      ],
      "metadata": {
        "id": "g4Jl2hriW3-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_default_test_metrics(num_classes, include_top2=True):\n",
        "  top2accuracy = TopKAccuracyMetric(k=2, num_classes=num_classes)\n",
        "  result = {'accuracy': (accuracy, accuracy_conversion_function, 'id')}\n",
        "  if include_top2:\n",
        "    result['top_2_accuracy'] = (top2accuracy, top_k_accuracy_conversion_function_predictions, top_k_accuracy_conversion_function_references)\n",
        "  return result"
      ],
      "metadata": {
        "id": "tDXExYygVc9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def compute_bert_score(text_dataset, tokenized_dataset, model, device, tokenizer, batch_size=32, lang='en', model_type=None, max_length=200, num_beams=4, num_candidates=1):\n",
        "  from evaluate import load\n",
        "  from tqdm.auto import tqdm\n",
        "  bert_score = load('bertscore')\n",
        "  dataset_length = len(text_dataset)\n",
        "  progress_bar = tqdm(range(dataset_length//batch_size + (dataset_length % batch_size != 0)))\n",
        "  start = 0\n",
        "  model.eval()\n",
        "  results = {\n",
        "      'precision': [],\n",
        "      'recall': [],\n",
        "      'f1': [],\n",
        "  }\n",
        "  while start < dataset_length:\n",
        "    end = min(start + batch_size, dataset_length)\n",
        "    batch = {\n",
        "        'references': text_dataset['question'][start:end],\n",
        "        'input_ids': tokenized_dataset['input_ids'][start:end].to(device),\n",
        "    }\n",
        "    with torch.no_grad():\n",
        "      predictions = model.generate(batch['input_ids'], max_length=max_length, num_beams=num_beams, num_return_sequences=num_candidates)\n",
        "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    bert_score_results = bert_score.compute(predictions=decoded_predictions, references=batch['references'], lang=lang, model_type=model_type)\n",
        "    for metric in ['precision', 'recall', 'f1']:\n",
        "      results[metric].append(np.mean(bert_score_results[metric]).item())\n",
        "    start = end\n",
        "    progress_bar.update(1)\n",
        "  return {k: (np.mean(v), np.std(v)) for k, v in results.items()}"
      ],
      "metadata": {
        "id": "dyz1hb6EJxH1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}